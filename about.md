---
layout: page
title: Hi, I'm Trenton.
permalink: /about/
---

<!--- ![Trenton B. Bricken](../images/TrentonBricken.jpg){:style="width: 200px; float: right; padding-left: 20px"}-->

I'm a Member of Technical Staff on the Alignment Science team at [Anthropic](https://www.anthropic.com/). I'm currently [enabling Claude to automatically audit and detect misalignment](https://alignment.anthropic.com/2025/automated-auditing/).

Information about me:
* I have a PhD in Systems Biology from Harvard. My thesis was on "Sparse Representations in Biological and Artificial Neural Networks" in the [Kreiman Lab](http://klab.tch.harvard.edu/publications/publications.html#sthash.dUJc7Kpv.dpbs) with support from the NSF Graduate Research Fellowship. I also spent time at the Berkeley [Redwood Center for Theoretical Neuroscience](https://redwood.berkeley.edu/) as a visiting researcher.
* I graduated from Duke University in May 2020 with a self-made major in "Minds and Machines: Biological and Artificial Intelligence". I was lucky to attend as a [Robertson Scholar](https://robertsonscholars.org/profiles/trenton-bricken/), which provided full funding during all four years, including summer experiences.
* At Duke, I spent a year doing research in [Dr. Michael Lynch's Lab](https://lynchlab.pratt.duke.edu) attempting to use machine learning to design new CRISPR guide RNAs for safer, more effective genome editing. Afterwards, I was affiliated with [Dr. Debora Marks's Lab](https://marks.hms.harvard.edu) at Harvard Medical School applying deep learning to [protein design](https://github.com/TrentBrick/learning-protein-landscapes).

I am involved in the movement/philosophy/set of ideas that is [Effective Altruism](https://www.effectivealtruism.org). I am also a fan of prediction markets and make public forecasts on Metaculus [here](https://www.metaculus.com/accounts/profile/118139/).
If the world was void of both interesting research questions and global catastrophic risks(!), you'd find me backpacking around the world with my [film camera](https://blitz-analog.github.io/). I still try to do this when I have time off and get the chance to travel somewhere cool.

Have any feedback for me? Please consider filling out [this](https://forms.gle/VcY3vQgkdf6c69dr7) anonymous feedback form so I can learn and grow.

## Publications (in reverse chronological order):

**Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise**<br>
Samuel R. Bowman, Megha Srivastava, Jon Kutasov, Rowan Wang, ***Trenton Bricken***, Benjamin Wright, Ethan Perez, and Nicholas Carlini<br>
Anthropic, August 2025<br>
[[paper](https://alignment.anthropic.com/2025/openai-findings/)] [[tweet-thread](https://x.com/sleepinyourhat/status/1960749648110395467)]

**Building and evaluating alignment auditing agents**<br>
***Trenton Bricken***, Rowan Wang, Sam Bowman, Euan Ong, Johannes Treutlein, Jeff Wu, Evan Hubinger, Samuel Marks<br>
Anthropic, July 2025<br>
[[paper](https://alignment.anthropic.com/2025/automated-auditing/)] [[tweet-thread](https://x.com/AnthropicAI/status/1948433493102403876)]

**On the Biology of a Large Language Model**<br>
Jack Lindsey†, Wes Gurnee\*, Emmanuel Ameisen\*, Brian Chen\*, Adam Pearce\*, Nicholas L. Turner\*, Craig Citro\*,
David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton,
***Trenton Bricken***, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson,
Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson*<br>
\† Lead Contributor<br>
\*(Core Contributor)<br>
Anthropic, March 2025<br>
[[paper](https://arxiv.org/abs/2503.10965)] [[blog-post](https://www.anthropic.com/research/tracing-thoughts-language-model)] [[tweet-thread](https://x.com/AnthropicAI/status/1905303835892990278)]

**Circuit Tracing: Revealing Computational Graphs in Language Models**<br>
Emmanuel Ameisen\*, Jack Lindsey\*, Adam Pearce\*, Wes Gurnee\*, Nicholas L. Turner\*, Brian Chen\*, Craig Citro\*,
David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton,
***Trenton Bricken***, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson,
Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson*<br>
\*(Core Contributor)<br>
Anthropic, March 2025<br>
[[paper](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)] [[blog-post](https://www.anthropic.com/research/tracing-thoughts-language-model)] [[tweet-thread](https://x.com/AnthropicAI/status/1905303835892990278)]

**Auditing Language Models for Hidden Objectives**<br>
Samuel Marks, Johannes Treutlein, ***Trenton Bricken***, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger<br>
Anthropic, March 2025<br>
[[paper](https://arxiv.org/abs/2503.10965)] [[tweet-thread](https://x.com/AnthropicAI/status/1900217234825634236)]

**Insights on Crosscoder Model Diffing**<br>
Siddharth Mishra-Sharma, ***Trenton Bricken***, Jack Lindsey, Adam Jermyn, Jonathan Marcus, Kelley Rivoire, Christopher Olah, Thomas Henighan<br>
Anthropic, February 2025<br>
[[paper](https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html)]

**Stage-Wise Model Diffing**<br>
***Trenton Bricken***, Siddharth Mishra-Sharma, Jonathan Marcus, Adam Jermyn, Christopher Olah, Kelley Rivoire, Thomas Henighan<br>
Anthropic, December 2024<br>
[[paper](https://transformer-circuits.pub/2024/model-diffing/index.html)] [[tweet-thread](https://x.com/TrentonBricken/status/1867102627588346350)]

**Using Dictionary Learning Features as Classifiers**<br>
***Trenton Bricken***, Jonathan Marcus, Siddharth Mishra-Sharma, Meg Tong, Ethan Perez, Mrinank Sharma, Kelley Rivoire, Thomas Henighan; edited by Adam Jermyn<br>
Anthropic, October 2024<br>
[[paper](https://transformer-circuits.pub/2024/features-as-classifiers/index.html)] [[tweet-thread](https://x.com/TrentonBricken/status/1846652943300436030)]

**Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet**<br>
Adly Templeton\*, Tom Conerly\*, Jonathan Marcus, Jack Lindsey, ***Trenton Bricken***, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, Tom Henighan<br>
\*(Core Contributor)<br>
Anthropic, May 2024<br>
[[paper](https://transformer-circuits.pub/2024/scaling-monosemanticity/)] [[blog-post](https://www.anthropic.com/news/mapping-mind-language-model)] [[tweet-thread](https://x.com/AnthropicAI/status/1792935506587656625)]

**Towards Monosemanticity: Decomposing Language Models With Dictionary Learning**<br>
***Trenton Bricken***\*, Adly Templeton\*, Joshua Batson\*, Brian Chen\*, Adam Jermyn\*, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Chris Olah<br>
\*(Core Contributor)<br>
Anthropic, October 2023<br>
[[paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html)] [[blog-post](https://www.anthropic.com/index/decomposing-language-models-into-understandable-components)] [[tweet-thread](https://x.com/AnthropicAI/status/1709986949711200722?s=20)]

**Emergence of Sparse Representations from Noise**<br>
***Trenton Bricken***\*, Rylan Schaeffer, Bruno Olshausen, Gabriel Kreiman<br>
\*(First author)<br>
ICML, May 2023<br>
[[paper](https://proceedings.mlr.press/v202/bricken23a/bricken23a.pdf)]

**Sparse Distributed Memory is a Continual Learner**<br>
***Trenton Bricken***\*, Xander Davies, Deepak Singh, Dmitry Krotov, Gabriel Kreiman<br>
\*(First author)<br>
ICLR, September 2022<br>
[[paper](https://arxiv.org/abs/2303.11934)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1639302453295476737?s=20)]

**Attention Approximates Sparse Distributed Memory**<br>
***Trenton Bricken***\*, Cengiz Pehlevan<br>
\*(First author)<br>
NeurIPS, December 2021<br>
[[paper](https://arxiv.org/abs/2111.05498)] [[blog-post](https://www.trentonbricken.com/Attention-Approximates-Sparse-Distributed-Memory/)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1458465726503784449?s=20)]<br>

MIT Center for Brains Minds+ Machines Talk:<br>

<div align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/THIIk7LR9_8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

*I gave a longer talk that enabled me to cover more of SDM's biological plausibilty to the VSA Online community [here](https://www.youtube.com/watch?v=DrJ2SRdMPUg).*

**High-content screening of coronavirus genes for innate immune suppression reveals enhanced potency of SARS-CoV-2 proteins.**<br>
Erika J Olson\*, David M Brown\*, Timothy Z Chang, Lin Ding, Tai L Ng, H. Sloane Weiss, Peter Koch, Yukiye Koide, Nathan Rollins, Pia Mach, Tobias Meisinger, ***Trenton Bricken***, Joshus Rollins, Yun Zhang, Colin Molloy, Yun Zhang, Briodget N Queenan, Timothy Mitchison, Debora Marks, Jeffrey C Way, John I Glass, Pamela A Silver<br>
\*(First authors)<br>
*bioRxiv, March 2021*<br>
[[preprint](https://www.biorxiv.org/content/10.1101/2021.03.02.433434v1)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1367141915666317312?s=20)]

**Computationally Optimized SARS-CoV-2 MHC Class I and II Vaccine Formulations Predicted to Target Human Haplotype Distributions.**<br>
Ge Liu\*, Brandon Carter\*, ***Trenton Bricken***, Siddhartha Jain, Mathias Viard, Mary Carrington, David K Gifford<br>
\*(First authors)<br>
*Cell Systems, July 2020*<br>
[[paper](https://www.cell.com/cell-systems/fulltext/S2405-4712%2820%2930238-6#%20)] [[code](https://github.com/gifford-lab/optivax)] [[preprint](https://www.biorxiv.org/content/10.1101/2020.05.16.088989v1)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1262407888842170370?s=20)]

My Google Scholar profile can be found [here](https://scholar.google.com/citations?user=CP6aLusAAAAJ&hl=en).

## Talks:

* [My PhD Defense](https://youtu.be/dEFn6nnoC-8) - March 2025
* [Stanford CS 25: Transformers United V2](https://web.stanford.edu/class/cs25/) - March 2023, [[public-link](https://www.youtube.com/watch?v=L4DC7e6g2iI&ab_channel=StanfordOnline)]
* [Vector Symbolic Architectures Webinar Series](https://sites.google.com/ltu.se/vsaonline) - September 2022, [[public-link](https://www.youtube.com/watch?v=DrJ2SRdMPUg)]
* [Redwood Center for Theoretical Neuroscience](https://redwood.berkeley.edu/) - May 2022
* [MIT Center for Brains Minds+ Machines](https://cbmm.mit.edu/) - November 2021, [[public-link](https://www.youtube.com/watch?v=THIIk7LR9_8&ab_channel=MITCBMM)]

## Podcasts:

* [Dwarkesh Podcast 2025 Episode](https://www.dwarkeshpatel.com/podcast):<br>

<div align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/64lXQP6cs5M?si=oKj2GeM6QqTXY30C" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

* [Dwarkesh Podcast 2024 Episode](https://www.dwarkeshpatel.com/podcast):<br>

<div align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/UTuuTTnjxMQ?si=h5v-7OqUiqnXq4xI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

## Past Projects (in reverse chronological order):

* [Upside Down Free Energy](https://github.com/TrentBrick/Upside-Down-Free-Energy) - Fall 2020 - Motivated by progress in "Upside Down" supervised reinforcement learning, I tried to connect it Friston's Free Energy Principle (FEP) and develop more hierarchical versions of FEP. This required first implementing benchmarks of the existing "Upside Down" RL algorithms (see next entry). I was starting to get somewhat interesting results but RL is [really hard](https://arxiv.org/pdf/1709.06560.pdf) and I started down the rabbit hole of [Sparse Distributed Memory](https://arxiv.org/abs/2111.05498). See the [GitHub repository](https://github.com/TrentBrick/Upside-Down-Free-Energy) for a draft PDF write up. Thanks to Beren Millidge and Alec Tschantz for their supervision and discussions about this project.

* [RewardConditionedUDRL](https://github.com/TrentBrick/RewardConditionedUDRL) - Fall 2020 - Open source codebase combining implementations of [Reward Conditioned Policies](https://arxiv.org/pdf/1912.13465.pdf) and [Training Agents using Upside-Down Reinforcement Learning](https://arxiv.org/abs/1912.02877). The former had no public implementation and the latter had a few implemented as Jupyter Notebooks but that had a number of issues I flagged eg. [here](https://github.com/jscriptcoder/Upside-Down-Reinforcement-Learning/issues/1) and [here](https://github.com/BY571/Upside-Down-Reinforcement-Learning/issues/4#event-3624848392). I hope this open source codebase will serve to both fully replicate the aforementioned papers and be used as a starting point for further research in the exciting domain of supervised RL.

* [SARS-CoV-2 mutation effects and 3D structure prediction from sequence covariation.](https://marks.hms.harvard.edu/sars-cov-2) - Summer 2020 - Collaborated with the Marks lab to help produce their SARS-CoV-2 mutation effect and 3D structure predictions using EVCouplings. <br>
Website: <https://marks.hms.harvard.edu/sars-cov-2>

* [RL Learning Byzantine Fault Tolerant (BFT) Consensus Protocols](https://github.com/TrentBrick/LearningConsensus) - Senior Year - Supervised by Dr. Kartik Nayak, final class project turned research project. Investigated the ability for deep reinforcement learning agents to discover and prove BFT consensus protocols. This was a great way to learn more about reinforcement learning but the tasks were too difficult for the agents to learn given the algorithms we were attempting to use. A write up of the project and uncleaned version of the codebase is available [here](https://github.com/TrentBrick/LearningConsensus).

* [Protein Generation and Optimization](https://github.com/TrentBrick/learning-protein-landscapes) - Supervised by Dr. Debora Marks's Lab as my Senior Thesis - This research was motivated by the promise of recent developments in our ability to predict protein functionality and the problem of finding novel sequences that maximize this prediction. We tried developing a new solution using invertible neural networks and variational inference to approximate the intractable distribution of any protein function predictor with reason to believe it would outperform Markov Chain Monte Carlo methods. My senior thesis write up of the work and where it seemed to succeed and fail can be found with the codebase [here](https://github.com/TrentBrick/learning-protein-landscapes).

* [PyTorch Discrete Normalizing Flows](https://github.com/TrentBrick/PyTorchDiscreteFlows) - Winter Break 2019 - Learning about Discrete Normalizing Flows from "Discrete Flows: Invertible Generative Models of Discrete Data", by Dustin Tran et al. <https://arxiv.org/pdf/1905.10347.pdf>, I tried implementing them using the coded provided in [edward2](https://github.com/google/edward2/tree/master/edward2/tensorflow/layers#4-reversible-layers) but found that [none of it worked](https://github.com/google/edward2/issues/148). I ended up porting all of the code into PyTorch, which required making a number of modifications and getting it working on a toy example. This repo as of October 2022 has 95 Github stars and two developers have reached out to collaborate and help me replicate the results.

* [Tail Free Sampling](https://trentbrick.github.io/Tail-Free-Sampling/) - Independent project, advice from friends and mentors - Developed a new method to sample sequences from autoregressive neural networks for open-ended sequence generation.

* [Primary and Tertiary Protein AutoEncoder](https://github.com/TrentBrick/PAE) - Final Class Project - Investigated if a deep AutoEncoder could learn the relationship between protein sequence and tertiary structure in order to then do either sequence or structure optimization in the latent space. It didn't work very well but I learned a lot!

* [Facebook Chatbot for Spaced Repetition Learning](https://github.com/TrentBrick/MMRY) - HackDuke 2016 - Spaced Repetition is both [wonderful and highly neglected](https://www.gwern.net/Spaced-repetition). Can we make it more popular and easy to do routinely using a Facebook Chatbot to both harass and motivate us? Got everything working! But there were always more bugs and this didn't solve the fundamental problem of spaced repetition learning still taking a huge amount of motivation. You could argue that presenting the cards over Messenger just created more distractions.  

### Other Locations on the Interwebs

I am pretty active on [Twitter](https://twitter.com/TrentonBricken). My DMs are open and you should feel free to reach out but I can't promise I'll be good at replying! I sometimes upload my film photography to [Instagram](https://www.instagram.com/blitz_analog) and to my [portfolio](https://blitz-analog.github.io/).
